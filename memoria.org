#+TITLE: Memoria
#+AUTHOR: Pedro Gómez Martín

#+LANGUAGE: spanish

#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage[a4paper, margin=2.5cm]{geometry}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[bottom]{footmisc}

#+LATEX_HEADER: \usepackage{amsmath}

#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usemintedstyle{solarized-light}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \restylefloat{figure}

* Mínimos cuadrados
** Enunciado
Se pide: hallar la parábola de regresión, la proyección (que denotamos
por $p$) del vector $b$ sobre el espacio de columnas de $A$ y el vector
de residuos $e$. Comprobar que si $A^t$ es la traspuesta de $A$ tenemos
$A^t e = 0$.

** Base teórica
Cuando tenemos una nube de puntos y queremos encontrar una recta que
pase por todos ellos, tenemos que resolver el siguiente sistema.
\begin{align}
Ax = b
\end{align}

Siendo la matriz $A$ la matriz que representa la variable
independiente de la curva

\begin{align}
A =
\begin{pmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m \\
1 & x_2 & x_2^2 & \cdots & x_2^m \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m
\end{pmatrix}
\end{align}

$x$ el vector que contiene los parámetros de la curva

\begin{align}
x =
\begin{pmatrix}
  \alpha_1 \\
  \alpha_2 \\
  \vdots \\
  \alpha_m
\end{pmatrix}
\end{align}

El vector $y$ las variables dependientes

\begin{align}
b =
\begin{pmatrix}
  y_1 \\
  y_2 \\
  \vdots \\
  y_n
\end{pmatrix}
\end{align}


\begin{align}
\end{align}

El problema con este método es que si queremos aproximar una curva, no
lo podemos hacer ya que si no coinciden los puntos, no tenemos una
solución para el sistema.

Para poder dar con una solución que tenga la mínima distancia a todos
los puntos, tendremos que encontrar un vector que esté en el espacio
de columnas $C \left( A \right)$ generado por $A$. Entonces, haciendo
una proyección del vector $\vec{b}$ sobre $C(A)$ tendremos un vector
que nos permita resolver el sistema de mínimos cuadrados
$Ax_0 = P_{C\left(A \right)} \left( b\right)$ por lo tanto
$b - P_{C\left(A \right)} \left( b\right)$ es ortogonal a
$C\left(A\right)$. Entonces, siendo $A_j$ la columna $j$ de $A$:

\begin{align}
    A_j \cdot \left( b - P_{C\left(A \right)} \left( b\right) \right) &= 0
    \quad \forall A_j \in A, \quad j = 1, ..., m \\
    A_j \cdot \left( b - Ax_0 \right) &= 0
\end{align}

Por lo tanto, en forma matricial sería:

\begin{align}
    A^{t} \cdot \left( b - Ax_0 \right) &= \vec{0}\\
    A^{t} b - A^{t} Ax_0 &= \vec{0}\\
    A^{t} Ax_0 &= A^{t} b\\
    x_0 &= \left( A^t \cdot A \right) ^ {-1} \cdot A^t b
\end{align}

Esta última ecuación nos dará la solución de mínimos cuadrados, si
expandimos las matrices, nos queda lo siguiente:

\begin{align}
\begin{pmatrix}
1      & 1      & 1      & \cdots & 1 \\
x_1    & x_2    & x_3    & \cdots & x_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_1^m  & x_2^m  & x_3^m  & \cdots & x_n^m
\end{pmatrix}
\begin{pmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m \\
1 & x_2 & x_2^2 & \cdots & x_2^m \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m
\end{pmatrix}
\begin{pmatrix}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_m
\end{pmatrix}
&=
\begin{pmatrix}
1      & 1      & 1      & \cdots & 1 \\
x_1    & x_2    & x_3    & \cdots & x_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_1^m  & x_2^m  & x_3^m  & \cdots & x_n^m
\end{pmatrix}
\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{pmatrix}\\
\begin{pmatrix}
n    & \sum\limits_{i=1}^n x_i & \cdots & \sum\limits_{i=1}^n x_i^m \\
\sum\limits_{i=1}^n x_i & \sum\limits_{i=1}^n x_i^2 & \cdots & \sum\limits_{i=1}^n x_i^{m+1} \\
\vdots & \vdots & \ddots & \vdots \\
\sum\limits_{i=1}^n x_i^m & \sum\limits_{i=1}^n x_i^{m+1} & \cdots & \sum\limits_{i=1}^n x_i^{m+n}
\end{pmatrix}
\begin{pmatrix}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_m
\end{pmatrix}
&=
\begin{pmatrix}
\sum\limits_{i=1}^n y_i \\
\vdots \\
\sum\limits_{i=1}^n x_i^m y_i \\
\end{pmatrix}
\end{align}

El vector de residuos se define como el vector resultante de restar el
punto por el que pasa la curva calculada al punto de los datos.
Definimos $e$ como el vector de resíduos y $b^\prime$ como el vector de
valores dados por la curva.

\begin{align}
e = b - b^\prime \ \ \ \ b^\prime = Ax_0
\end{align}

Por lo tanto, nos queda que el vector de residuos es:

\begin{align}
e = b - Ax_0
\end{align}

Por definición $e$ es $b$ menos la proyección de $b$ en el espacio de
columnas de $A$. Por lo tanto, si multiplicamos $A^t \cdot e$, lo que
obtenemos es $\vec{0}$ ya que $e$ es ortogonal a $C(A)$.

** Práctica
*** Cálculo de los parámetros de la parábola de regresión
Con este código calculamos el vector $x_0$ y lo guardamos en la
variable ans

#+name: figura-1
#+begin_src octave :exports both :cache yes
pkg load symbolic;
figure(1, "visible", "off");

x = [-3;-2;-1;0;1;2;3];
b = [5; 3; 2; 1; 2; 3; 5]

A = [1,-3,9;1,-2,4;1,-1,1;1,0,0;1,1,1;1,2,4;1,3,9];
B = transpose(A) * A;
res = inverse(B) * transpose(A) * b;
ans = res
#+end_src

El valor de la variable ans es para este bloque de código es:

#+RESULTS: figura-1
|  1.380952380952381 |
|                  0 |
| 0.4047619047619048 |

*** Cálculo de la proyección
En este bloque de código calculamos la proyección de $b$ sobre el
espacio de columnas de $A$:

#+name: figura-2
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-1>>
ans = A * res
#+end_src

#+RESULTS: figura-2
| 5.023809523809524 |
|                 3 |
| 1.785714285714286 |
| 1.380952380952381 |
| 1.785714285714286 |
|                 3 |
| 5.023809523809524 |

*** Cálculo del vector de residuos
En esta parte hacemos el cálculo de $e$ por la traspuesta de $A$

#+name: figura-3
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-1>>
E = b - A * res;
ans = transpose(A) * E
#+end_src

El valor de la variable ans es para este bloque de código es:

#+RESULTS: figura-3
| -6.661338147750939e-16 |
|                      0 |
|  8.881784197001252e-16 |

Como podemos observar, el resultado de $A^te$ es $\vec{0}$ (en este
caso muy cercano a cero por el error al multiplicar números en coma
flotante)

* Matriz de proyección
** Enunciado
Construir la matriz $P$ (estamos proyectando sobre el espacio de
columnas de la matriz $A$, que tiene dimensión $r = 3$) y comprobar que
el producto $Pb$ nos da la proyección $p$ hallada en el apartado (a).
Hallar la traza de $P$ y explicar qué significa el resultado (esto
quedará más claro con lo que veremos después…).

** Base teórica
Para construir la matriz de proyección primero tenemos que entender el
significado y a partir de ahí construir la matriz.

Podemos entender la proyección como la operación que utilizamos para
"ver" la sombra que proyecta un vector sobre un subespacio. Si
consideramos $S$ y $S^\perp$ como subespacios de $\mathbb{R}^m$ que
cumplen las siguientes propiedades:

1. $S \oplus S^\perp = \mathbb{R}^m$
2. $S \cap S^\perp = \emptyset$

Entonces podemos descomponer un vector $u$ de la siguiente forma
$u = v + w$ estando $v \in S$ y $w \in S^\perp$, con esta
descomposición definimos el resultado de la proyección del vector
$u$ sobre el subespacio $S$ como el vector $v$.

Con el producto escalar usual en $\mathbb{R}^m$
($\left\langle x,y \right\rangle =  x^t y$) si vamos a proyectar
$b \in \mathbb{R}^m$ sobre $S \subset \mathbb{R}^m$
($\text{dim}(S) < m$) definimos la matriz $A \in \mathcal{M}_{m \times n}$
como la matriz que contiene en sus columnas los vectores que conforman
la base de $S$.

Entonces, la proyección de $b$ sobre $\mathcal{C}(A)$ será $p=Ax$, para
$x \in \mathbb{R}^n$ por lo tanto $b - p = b - Ax$ debe de ser
ortogonal a $\mathcal{C}(A)$, lo cual implica:

\begin{align}
A^t\left(b - Ax\right) &= \vec{0}\\
A^t b &= A^t Ax\\
x &= (A^tA)^{-1}A^t b
\end{align}

Si $x$ resuelve el sistema anterior, entonces la proyección viene dada
por $p=Ax$ y la matriz de proyección $P$ es $A(A^tA)^{-1}A^t$.

\begin{align}
p &= Ax\\
p &= A \cdot (A^t A)^{-1}A^t b\\
p &= P \cdot b
\end{align}

** Práctica
*** Cálculo de la matriz $P$
La función ~orth~ nos permite encontrar la base ortonormal del espacio
de columnas de una matriz. Para encontrar la matriz $P$ simplemente
implementamos la ecuación hallada en la figura anterior y para poder
mostrarla en un formato más legible utilizamos ~eval~ y ~mat2str~ para
redondear todos los números de la matriz.

#+name: figura-4
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-1>>
Aa = orth(A);
L = inverse(transpose(Aa) * Aa) * transpose(Aa);
P = Aa * L;
ans = eval(mat2str(P, 3))
#+end_src

Aquí podemos ver la matriz $P$:

#+RESULTS: figura-4
|   0.762 |    0.357 | 0.0714 | -0.0952 | -0.143 |  -0.0714 |   0.119 |
|   0.357 |    0.286 |  0.214 |   0.143 | 0.0714 | 6.25e-17 | -0.0714 |
|  0.0714 |    0.214 |  0.286 |   0.286 |  0.214 |   0.0714 |  -0.143 |
| -0.0952 |    0.143 |  0.286 |   0.333 |  0.286 |    0.143 | -0.0952 |
|  -0.143 |   0.0714 |  0.214 |   0.286 |  0.286 |    0.214 |  0.0714 |
| -0.0714 | 8.33e-17 | 0.0714 |   0.143 |  0.214 |    0.286 |   0.357 |
|   0.119 |  -0.0714 | -0.143 | -0.0952 | 0.0714 |    0.357 |   0.762 |

*** Cálculo de la proyección de $b$
Para obtener la proyección de $b$ sobre $S$ tenemos que multiplicar
por la matriz que hemos calculado antes:

#+name: figura-5
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-4>>
ans = P * b
#+end_src

Como podemos observar, el resultado coincide:

#+RESULTS: figura-5
| 5.023809523809526 |
|                 3 |
| 1.785714285714286 |
| 1.380952380952382 |
| 1.785714285714286 |
|                 3 |
| 5.023809523809524 |

*** Cálculo de la traza de $P$
Para obtener la traza de $P$ tenemos primero que extraer la diagonal y
después sumar todos los elementos:

#+name: figura-6
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-4>>
ans = trace(P)
#+end_src

Aquí podemos ver que la traza es:

#+RESULTS[f083125bc18d703b2b7a7fe287bd84445f331daf]: figura-6
: 3

La traza de la matriz de proyección es 3 dado que la base del
subespacio tiene rango 3

* Complemento ortogonal
** Enunciado
Hallar (mediante la eliminación gaussiana, es decir con las
herramientas del capítulo 1) una base del complemento ortogonal de $U$
($U^{\perp}$).
Este subespacio de $E$ tendrá dimensión $7 - 3 = 4$; llamaremos a los
vectores de esa base $w_1$, $w_2$, $w_3$, $w_4$).  Hacerlo paso a paso,
pero puede usarse para comprobar la instrucción de MATLAB
~null(AT, 'r')~ ('r' para una solución racional).

** Base teórica
Sabemos que el complemento ortogonal de un subespacio vectorial
$U^{\perp}$ tiene la característica de tener el producto escalar de
todos sus vectores con los vectores que componen el otro subespacio
$U$ igual a 0.

\begin{align}
\left\langle u , v \right\rangle = 0\ \ \ \ \forall u \in U,\ \ \forall v \in U^{\perp}
\end{align}

Cuando son ortogonales, se indica de la siguiente forma:

\begin{align}
U \perp U^{\perp}
\end{align}

Por lo tanto, con esta definición, podemos describir el proceso
mediante el cual obtenemos $U^{\perp}$ a partir de $U$. Si
consideramos $A$ como la matriz que contiene los vectores que
componen $U$ y sacamos de ahí el espacio nulo, obtendremos
$U^{\perp}$.

** Práctica
Para hallar el complemento orgonal del subespacio plasmado en la
matriz $A$, primero tenemos que realizar la eliminación gaussiana en
la traspuesta de dicha matriz:

#+name: figura-7
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-1>>
m1 = transpose(A)
ans = transpose(A)
#+end_src

#+RESULTS[9d2ea67c926cbaeef1223e56a5d807289f410398]: figura-7
|  1 |  1 |  1 | 1 | 1 | 1 | 1 |
| -3 | -2 | -1 | 0 | 1 | 2 | 3 |
|  9 |  4 |  1 | 0 | 1 | 4 | 9 |

Una vez hecha la traspuesta, procedemos a hacer manualmente la
eliminación gaussiana. Para eso, tenemos que construir la matriz que
le vamos a restar a la matriz inicial. En la fila que vamos a usar
para calcular el resto ponemos todo 0, en las siguientes extraemos
la primera fila y la multiplicamos por el valor del escalar en la
primera columna de esa fila.

#+name: figura-8
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-7>>
m2 = m1 - [0 0 0 0 0 0 0; m1(1, :) * m1(2,1); m1(1,:) * m1(3,1)]
ans = m2
#+end_src

#+RESULTS[264004359bb215d57220cac6f3cc37526965d04d]: figura-8
| 1 |  1 |  1 |  1 |  1 |  1 | 1 |
| 0 |  1 |  2 |  3 |  4 |  5 | 6 |
| 0 | -5 | -8 | -9 | -8 | -5 | 0 |

Para la siguiente iteración del algoritmo, tanto la primera como la
segunda fila son puestas a cero y para la tercera fila repetimos el
proceso, pero esta vez con la segunda fila y el escalar en la segunda
columna.

#+name: figura-9
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-8>>
m3 = m2 - [0 0 0 0 0 0 0; 0 0 0 0 0 0 0; m2(2,:) * m2(3,2)]
ans = m3
#+end_src

#+RESULTS[6fcd6cfcc8451a58270a9f5f6c72a5771cb95e1a]: figura-9
| 1 | 1 | 1 | 1 |  1 |  1 |  1 |
| 0 | 1 | 2 | 3 |  4 |  5 |  6 |
| 0 | 0 | 2 | 6 | 12 | 20 | 30 |

Una vez tenemos la matriz $M_3$, que es la matriz $A$ a la que le
hemos aplicado la eliminación gaussiana, procedemos a resolver el
sistema $M_3 \cdot x = \vec{0}$:

#+name: figura-10
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-9>>
A = horzcat(m3(:,1), m3(:,2), m3(:,3))
X0 = [linsolve(A, m3(:,4)); 1; 0; 0; 0]
ans = X0
#+end_src

#+RESULTS[58c4fb606f79442788b47fbedc1cf771826b5377]: figura-10
|  1 |
| -3 |
|  3 |
|  1 |
|  0 |
|  0 |
|  0 |

#+name: figura-11
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-9>>
A = horzcat(m3(:,1), m3(:,2), m3(:,3))
X1 = [linsolve(A, m3(:,5)); 0; 1; 0; 0]
ans = X1
#+end_src

#+RESULTS[09a36cc9812c315136e516e48aa4a452239634b7]: figura-11
|  3 |
| -8 |
|  6 |
|  0 |
|  1 |
|  0 |
|  0 |

#+name: figura-12
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-9>>
A = horzcat(m3(:,1), m3(:,2), m3(:,3))
X2 = [linsolve(A, m3(:,6)); 0; 0; 1; 0]
ans = X2
#+end_src

#+RESULTS[2bbbd253b2bb3e3e4c5eaa725faa94659664f7cc]: figura-12
|   6 |
| -15 |
|  10 |
|   0 |
|   0 |
|   1 |
|   0 |

#+name: figura-13
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-9>>
A = horzcat(m3(:,1), m3(:,2), m3(:,3))
X3 = [linsolve(A, m3(:,7)); 0; 0; 0; 1]
ans = X3
#+end_src

Por lo tanto, el subespacio $U^{\perp}$ está compuesto por los
vectores que conforman la siguiente matriz:

#+name: figura-13
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-10>>
<<figura-11>>
<<figura-12>>
<<figura-13>>
ans = horzcat(X0, X1, X2, X3)
#+end_src

#+RESULTS[4fd2ca2242592dd5c5c0ef772e5082ef29569173]: figura-13
|  1 |  3 |   6 |  10 |
| -3 | -8 | -15 | -24 |
|  3 |  6 |  10 |  15 |
|  1 |  0 |   0 |   0 |
|  0 |  1 |   0 |   0 |
|  0 |  0 |   1 |   0 |
|  0 |  0 |   0 |   1 |

* Aplicación lineal
** Enunciado
Consideramos la aplicación lineal $f$ de $E$ en $E$ tal que a un
vector $b$ de $E$ le hace corresponder su proyección sobre $U$
(el espacio de columnas de $A$). La matriz de proyección $P$ es la
matriz de la aplicación lineal $f$ respecto a la base canónica de $E$.
La imagen de $b$ es $P_b$.  Ahora pedimos:

Hallar $M$, la matriz coordenada de esta aplicación lineal respecto a
la base
\begin{align*}
B = \left\lbrace v_1,v_2,v_3, w_1, w_2, w_3, w_4 \right\rbrace
\end{align*}

Una vez hallada $M$, se pide usar $M$ para hallar $f(b_2)$, es decir la
imagen del vector
\begin{align*}
b2 = \left\lbrace -9, 64, -33, 5, 0, 5, 10 \right\rbrace
\end{align*}
