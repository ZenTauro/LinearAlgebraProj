#+TITLE: Memoria
#+AUTHOR: Pedro Gómez Martín

#+LANGUAGE: spanish

#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage[a4paper, margin=2.5cm]{geometry}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[bottom]{footmisc}

#+LATEX_HEADER: \usepackage{amsmath}

#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usemintedstyle{solarized-light}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \restylefloat{figure}

#+LATEX_HEADER: \usepackage{endnotes}
#+LATEX_HEADER: \renewcommand{\footnote}{\endnote}

\setlength{\parskip}{0.5em}

\pagebreak
* Mínimos cuadrados
** Enunciado
Se pide: hallar la parábola de regresión, la proyección (que denotamos
por $p$) del vector $b$ sobre el espacio de columnas de $A$ y el vector
de residuos $e$. Comprobar que si $A^t$ es la traspuesta de $A$ tenemos
$A^t e = 0$.

** Base teórica
Cuando tenemos una nube de puntos y queremos encontrar una recta que
pase por todos ellos, tenemos que resolver el siguiente sistema.
\begin{align}
Ax = b
\end{align}

Siendo la matriz $A$ la matriz que representa la variable
independiente de la curva

\begin{align}
A =
\begin{pmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m \\
1 & x_2 & x_2^2 & \cdots & x_2^m \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m
\end{pmatrix}
\end{align}

$x$ el vector que contiene los parámetros de la curva

\begin{align}
x =
\begin{pmatrix}
  \alpha_1 \\
  \alpha_2 \\
  \vdots \\
  \alpha_m
\end{pmatrix}
\end{align}

El vector $y$ las variables dependientes

\begin{align}
b =
\begin{pmatrix}
  y_1 \\
  y_2 \\
  \vdots \\
  y_n
\end{pmatrix}
\end{align}


\begin{align}
\end{align}

El problema con este método es que si queremos aproximar una curva, no
lo podemos hacer, ya que si no coinciden los puntos, no tenemos una
solución para el sistema.

Para poder dar con una solución que tenga la mínima distancia a todos
los puntos, tendremos que encontrar un vector que esté en el espacio
de columnas $C \left( A \right)$ generado por $A$. Entonces, haciendo
una proyección del vector $\vec{b}$ sobre $C(A)$ tendremos un vector
que nos permita resolver el sistema de mínimos cuadrados
$Ax_0 = P_{C\left(A \right)} \left( b\right)$ por lo tanto
$b - P_{C\left(A \right)} \left( b\right)$ es ortogonal a
$C\left(A\right)$. Entonces, siendo $A_j$ la columna $j$ de $A$:

\begin{align}
    A_j \cdot \left( b - P_{C\left(A \right)} \left( b\right) \right) &= 0
    \quad \forall A_j \in A, \quad j = 1, ..., m \\
    A_j \cdot \left( b - Ax_0 \right) &= 0
\end{align}

Por lo tanto, en forma matricial sería:

\begin{align}
    A^{t} \cdot \left( b - Ax_0 \right) &= \vec{0}\\
    A^{t} b - A^{t} Ax_0 &= \vec{0}\\
    A^{t} Ax_0 &= A^{t} b\\
    x_0 &= \left( A^t \cdot A \right) ^ {-1} \cdot A^t b
\end{align}

Esta última ecuación nos dará la solución de mínimos cuadrados, si
expandimos las matrices, nos queda lo siguiente:

\begin{align}
\begin{pmatrix}
1      & 1      & 1      & \cdots & 1 \\
x_1    & x_2    & x_3    & \cdots & x_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_1^m  & x_2^m  & x_3^m  & \cdots & x_n^m
\end{pmatrix}
\begin{pmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m \\
1 & x_2 & x_2^2 & \cdots & x_2^m \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m
\end{pmatrix}
\begin{pmatrix}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_m
\end{pmatrix}
&=
\begin{pmatrix}
1      & 1      & 1      & \cdots & 1 \\
x_1    & x_2    & x_3    & \cdots & x_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_1^m  & x_2^m  & x_3^m  & \cdots & x_n^m
\end{pmatrix}
\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{pmatrix}\\
\begin{pmatrix}
n    & \sum\limits_{i=1}^n x_i & \cdots & \sum\limits_{i=1}^n x_i^m \\
\sum\limits_{i=1}^n x_i & \sum\limits_{i=1}^n x_i^2 & \cdots & \sum\limits_{i=1}^n x_i^{m+1} \\
\vdots & \vdots & \ddots & \vdots \\
\sum\limits_{i=1}^n x_i^m & \sum\limits_{i=1}^n x_i^{m+1} & \cdots & \sum\limits_{i=1}^n x_i^{m+n}
\end{pmatrix}
\begin{pmatrix}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_m
\end{pmatrix}
&=
\begin{pmatrix}
\sum\limits_{i=1}^n y_i \\
\vdots \\
\sum\limits_{i=1}^n x_i^m y_i \\
\end{pmatrix}
\end{align}

El vector de residuos se define como el vector resultante de restar el
punto por el que pasa la curva calculada al punto de los datos.
Definimos $e$ como el vector de residuos y $b^\prime$ como el vector de
valores dados por la curva.

\begin{align}
e = b - b^\prime \ \ \ \ b^\prime = Ax_0
\end{align}

Por lo tanto, nos queda que el vector de residuos es:

\begin{align}
e = b - Ax_0
\end{align}

Por definición $e$ es $b$ menos la proyección de $b$ en el espacio de
columnas de $A$. Por lo tanto, si multiplicamos $A^t \cdot e$, lo que
obtenemos es $\vec{0}$, ya que $e$ es ortogonal a $C(A)$.

** Práctica
*** Cálculo de los parámetros de la parábola de regresión
Con este código calculamos el vector $x_0$ y lo guardamos en la
variable ans

#+name: figura-1
#+begin_src octave :exports both :cache yes
pkg load symbolic;
figure(1, "visible", "off");

x = [-3;-2;-1;0;1;2;3];
b = [5; 3; 2; 1; 2; 3; 5]

A = [1,-3,9;1,-2,4;1,-1,1;1,0,0;1,1,1;1,2,4;1,3,9];
B = transpose(A) * A;
res = inverse(B) * transpose(A) * b;
ans = res
#+end_src

El valor de la variable ans es para este bloque de código es:

#+RESULTS: figura-1
|  1.380952380952381 |
|                  0 |
| 0.4047619047619048 |

*** Cálculo de la proyección
En este bloque de código calculamos la proyección de $b$ sobre el
espacio de columnas de $A$:

#+name: figura-2
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-1>>
ans = A * res
#+end_src

#+RESULTS: figura-2
| 5.023809523809524 |
|                 3 |
| 1.785714285714286 |
| 1.380952380952381 |
| 1.785714285714286 |
|                 3 |
| 5.023809523809524 |

*** Cálculo del vector de residuos
En esta parte hacemos el cálculo de $e$ por la traspuesta de $A$

#+name: figura-3
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-1>>
E = b - A * res;
ans = transpose(A) * E
#+end_src

El valor de la variable ans es para este bloque de código es:

#+RESULTS: figura-3
| -6.661338147750939e-16 |
|                      0 |
|  8.881784197001252e-16 |

Como podemos observar, el resultado de $A^te$ es $\vec{0}$ (en este
caso muy cercano a cero por el error al multiplicar números en coma
flotante)


\pagebreak
* Matriz de proyección
** Enunciado
Construir la matriz $P$ (estamos proyectando sobre el espacio de
columnas de la matriz $A$, que tiene dimensión $r = 3$) y comprobar que
el producto $Pb$ nos da la proyección $p$ hallada en el apartado (a).
Hallar la traza de $P$ y explicar qué significa el resultado (esto
quedará más claro con lo que veremos después…).

** Base teórica
Para construir la matriz de proyección primero tenemos que entender el
significado y a partir de ahí construir la matriz.

Podemos entender la proyección como la operación que utilizamos para
"ver" la sombra que proyecta un vector sobre un subespacio. Si
consideramos $S$ y $S^\perp$ como subespacios de $\mathbb{R}^m$ que
cumplen las siguientes propiedades:

1. $S \oplus S^\perp = \mathbb{R}^m$
2. $S \cap S^\perp = \emptyset$

Entonces podemos descomponer un vector $u$ de la siguiente forma
$u = v + w$ estando $v \in S$ y $w \in S^\perp$, con esta
descomposición definimos el resultado de la proyección del vector
$u$ sobre el subespacio $S$ como el vector $v$.

Con el producto escalar usual en $\mathbb{R}^m$
($\left\langle x,y \right\rangle =  x^t y$) si vamos a proyectar
$b \in \mathbb{R}^m$ sobre $S \subset \mathbb{R}^m$
($\text{dim}(S) < m$) definimos la matriz $A \in \mathcal{M}_{m \times n}$
como la matriz que contiene en sus columnas los vectores que conforman
la base de $S$.

Entonces, la proyección de $b$ sobre $\mathcal{C}(A)$ será $p=Ax$, para
$x \in \mathbb{R}^n$ por lo tanto $b - p = b - Ax$ debe de ser
ortogonal a $\mathcal{C}(A)$, lo cual implica:

\begin{align}
A^t\left(b - Ax\right) &= \vec{0}\\
A^t b &= A^t Ax\\
x &= (A^tA)^{-1}A^t b
\end{align}

Si $x$ resuelve el sistema anterior, entonces la proyección viene dada
por $p=Ax$ y la matriz de proyección $P$ es $A(A^tA)^{-1}A^t$.

\begin{align}
p &= Ax\\
p &= A \cdot (A^t A)^{-1}A^t b\\
p &= P \cdot b
\end{align}

** Práctica
*** Cálculo de la matriz $P$
La función ~orth~ nos permite encontrar la base ortonormal del espacio
de columnas de una matriz. Para encontrar la matriz $P$ simplemente
implementamos la ecuación hallada en la figura anterior y para poder
mostrarla en un formato más legible utilizamos ~eval~ y ~mat2str~ para
redondear todos los números de la matriz.

#+name: figura-4
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-1>>
Aa = orth(A);
L = inverse(transpose(Aa) * Aa) * transpose(Aa);
P = Aa * L;
ans = eval(mat2str(P, 3))
#+end_src

Aquí podemos ver la matriz $P$:

#+RESULTS: figura-4
|   0.762 |    0.357 | 0.0714 | -0.0952 | -0.143 |  -0.0714 |   0.119 |
|   0.357 |    0.286 |  0.214 |   0.143 | 0.0714 | 6.25e-17 | -0.0714 |
|  0.0714 |    0.214 |  0.286 |   0.286 |  0.214 |   0.0714 |  -0.143 |
| -0.0952 |    0.143 |  0.286 |   0.333 |  0.286 |    0.143 | -0.0952 |
|  -0.143 |   0.0714 |  0.214 |   0.286 |  0.286 |    0.214 |  0.0714 |
| -0.0714 | 8.33e-17 | 0.0714 |   0.143 |  0.214 |    0.286 |   0.357 |
|   0.119 |  -0.0714 | -0.143 | -0.0952 | 0.0714 |    0.357 |   0.762 |

*** Cálculo de la proyección de $b$
Para obtener la proyección de $b$ sobre $S$ tenemos que multiplicar
por la matriz que hemos calculado antes:

#+name: figura-5
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-4>>
ans = P * b
#+end_src

Como podemos observar, el resultado coincide:

#+RESULTS: figura-5
| 5.023809523809526 |
|                 3 |
| 1.785714285714286 |
| 1.380952380952382 |
| 1.785714285714286 |
|                 3 |
| 5.023809523809524 |

*** Cálculo de la traza de $P$
Para obtener la traza de $P$ tenemos primero que extraer la diagonal y
después sumar todos los elementos:

#+name: figura-6
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-4>>
ans = trace(P)
#+end_src

Aquí podemos ver que la traza es:

#+RESULTS[f083125bc18d703b2b7a7fe287bd84445f331daf]: figura-6
: 3

La traza de la matriz de proyección es 3 dado que la base del
subespacio tiene rango 3

\pagebreak
* Complemento ortogonal
** Enunciado
Hallar (mediante la eliminación gaussiana, es decir con las
herramientas del capítulo 1) una base del complemento ortogonal de $U$
($U^{\perp}$).
Este subespacio de $E$ tendrá dimensión $7 - 3 = 4$; llamaremos a los
vectores de esa base $w_1$, $w_2$, $w_3$, $w_4$).  Hacerlo paso a paso,
pero puede usarse para comprobar la instrucción de MATLAB
~null(AT, 'r')~ ('r' para una solución racional).

** Base teórica
Sabemos que el complemento ortogonal de un subespacio vectorial
$U^{\perp}$ tiene la característica de tener el producto escalar de
todos sus vectores con los vectores que componen el otro subespacio
$U$ igual a 0.

\begin{align}
\left\langle u , v \right\rangle = 0\ \ \ \ \forall u \in U,\ \ \forall v \in U^{\perp}
\end{align}

Cuando son ortogonales, se indica de la siguiente forma:

\begin{align}
U \perp U^{\perp}
\end{align}

Por lo tanto, con esta definición, podemos describir el proceso
mediante el cual obtenemos $U^{\perp}$ a partir de $U$. Si
consideramos $A$ como la matriz que contiene los vectores que
componen $U$ y sacamos de ahí el espacio nulo, obtendremos
$U^{\perp}$.

** Práctica
Para hallar el complemento ortogonal del subespacio plasmado en la
matriz $A$, primero tenemos que realizar la eliminación gaussiana en
la traspuesta de dicha matriz:

#+name: figura-7
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-1>>
m1 = transpose(A)
ans = transpose(A)
#+end_src

#+RESULTS[9d2ea67c926cbaeef1223e56a5d807289f410398]: figura-7
|  1 |  1 |  1 | 1 | 1 | 1 | 1 |
| -3 | -2 | -1 | 0 | 1 | 2 | 3 |
|  9 |  4 |  1 | 0 | 1 | 4 | 9 |

Una vez hecha la traspuesta, procedemos a hacer manualmente la
eliminación gaussiana. Para eso, tenemos que construir la matriz que
le vamos a restar a la matriz inicial. En la fila que vamos a usar
para calcular el resto ponemos todo 0, en las siguientes extraemos
la primera fila y la multiplicamos por el valor del escalar en la
primera columna de esa fila.

#+name: figura-8
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-7>>
m2 = m1 - [0 0 0 0 0 0 0; m1(1, :) * m1(2,1); m1(1,:) * m1(3,1)]
ans = m2
#+end_src

#+RESULTS[264004359bb215d57220cac6f3cc37526965d04d]: figura-8
| 1 |  1 |  1 |  1 |  1 |  1 | 1 |
| 0 |  1 |  2 |  3 |  4 |  5 | 6 |
| 0 | -5 | -8 | -9 | -8 | -5 | 0 |

Para la siguiente iteración del algoritmo, tanto la primera como la
segunda fila son puestas a cero y para la tercera fila repetimos el
proceso, pero esta vez con la segunda fila y el escalar en la segunda
columna.

#+name: figura-9
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-8>>
m3 = m2 - [0 0 0 0 0 0 0; 0 0 0 0 0 0 0; m2(2,:) * m2(3,2)]
ans = m3
#+end_src

#+RESULTS[6fcd6cfcc8451a58270a9f5f6c72a5771cb95e1a]: figura-9
| 1 | 1 | 1 | 1 |  1 |  1 |  1 |
| 0 | 1 | 2 | 3 |  4 |  5 |  6 |
| 0 | 0 | 2 | 6 | 12 | 20 | 30 |

Una vez tenemos la matriz $M_3$, que es la matriz $A$ a la que le
hemos aplicado la eliminación gaussiana, procedemos a resolver el
sistema $M_3 \cdot x = \vec{0}$:

#+name: figura-10
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-9>>
A = horzcat(m3(:,1), m3(:,2), m3(:,3))
X0 = [linsolve(A, m3(:,4)); 1; 0; 0; 0]
ans = X0
#+end_src

#+RESULTS[58c4fb606f79442788b47fbedc1cf771826b5377]: figura-10
|  1 |
| -3 |
|  3 |
|  1 |
|  0 |
|  0 |
|  0 |

#+name: figura-11
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-9>>
A = horzcat(m3(:,1), m3(:,2), m3(:,3))
X1 = [linsolve(A, m3(:,5)); 0; 1; 0; 0]
ans = X1
#+end_src

#+RESULTS[09a36cc9812c315136e516e48aa4a452239634b7]: figura-11
|  3 |
| -8 |
|  6 |
|  0 |
|  1 |
|  0 |
|  0 |

#+name: figura-12
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-9>>
A = horzcat(m3(:,1), m3(:,2), m3(:,3))
X2 = [linsolve(A, m3(:,6)); 0; 0; 1; 0]
ans = X2
#+end_src

#+RESULTS[2bbbd253b2bb3e3e4c5eaa725faa94659664f7cc]: figura-12
|   6 |
| -15 |
|  10 |
|   0 |
|   0 |
|   1 |
|   0 |

#+name: figura-13
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-9>>
A = horzcat(m3(:,1), m3(:,2), m3(:,3))
X3 = [linsolve(A, m3(:,7)); 0; 0; 0; 1]
ans = X3
#+end_src

Por lo tanto, el subespacio $U^{\perp}$ está compuesto por los
vectores que conforman la siguiente matriz:

#+name: figura-14
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-10>>
<<figura-11>>
<<figura-12>>
<<figura-13>>
ans = horzcat(X0, X1, X2, X3)
#+end_src

#+RESULTS[4fd2ca2242592dd5c5c0ef772e5082ef29569173]: figura-14
|  1 |  3 |   6 |  10 |
| -3 | -8 | -15 | -24 |
|  3 |  6 |  10 |  15 |
|  1 |  0 |   0 |   0 |
|  0 |  1 |   0 |   0 |
|  0 |  0 |   1 |   0 |
|  0 |  0 |   0 |   1 |

\pagebreak
* Aplicación lineal
** Enunciado
Consideramos la aplicación lineal $f$ de $E$ en $E$ tal que a un
vector $b$ de $E$ le hace corresponder su proyección sobre $U$
(el espacio de columnas de $A$). La matriz de proyección $P$ es la
matriz de la aplicación lineal $f$ respecto a la base canónica de $E$.
La imagen de $b$ es $P_b$.  Ahora pedimos:

Hallar $M$, la matriz coordenada de esta aplicación lineal respecto a
la base
\begin{align*}
B = \left\lbrace v_1,v_2,v_3, w_1, w_2, w_3, w_4 \right\rbrace
\end{align*}

Una vez hallada $M$, se pide usar $M$ para hallar $f(b_2)$, es decir la
imagen del vector
\begin{align*}
b_2 =
\begin{pmatrix}
-9\\ 64\\ -33\\ 5\\ 0\\ 5\\ 10
\end{pmatrix}
\end{align*}

** Base teórica
*** Definición y estudio del concepto de Aplicación Lineal
Una aplicación lineal entre dos espacios, por definición, es lineal.
Si recordamos lo que implica la linealidad podremos encontrar el
método para hallar la matriz que nos sirva para pasar un vector de
un espacio al otro obteniendo el mismo resultado que si se hiciera la
aplicación lineal a mano.

Si tenemos dos espacios $E$ y $F$, una aplicación lineal
$f:E \rightarrow F$, $a$ y $b$ son vectores en el espacio $E$ y
$\lambda$ un escalar[fn:1]. Se cumplen las siguientes propiedades:
\begin{align}
&f(a + b) = f(a) + f(b)\\
&f(\lambda a) = \lambda f(a)
\end{align}

Entonces, si $B_E = \left\lbrace u_1, u_2, \cdots , u_n \right\rbrace$
es una base del espacio $E$ y
$B_F = \left\lbrace v_1, v_2, \cdots, v_m \right\rbrace$ es una base de
$F$, se cumple lo siguiente:
\begin{align}
a =
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{pmatrix}
= x_1 u_1 + x_2 u_x + \cdots x_n u_n
\end{align}

Esto es expresar el vector $a$ como combinación lineal de los vectores
que componen $B_E$ o, dicho de otra forma, las coordenadas del vector
$a$ sobre la base $B_E$.

Como $f$ es lineal:
\begin{align}
f(a) &= f(x_1 u_1 + x_2 u_2 + \cdots + x_n u_n)\\
     &= x_1 f(u_1) + x_2 f(u_2) + \cdots + x_n f(u_n)
\end{align}

A partir de esta conclusión, podemos extraer la matriz $A$:
\begin{align}
A = \left( f(u_1) | f(u_2) | \cdots | f(u_n) \right)
\end{align}

De la misma forma que hemos expresado $a$ como suma de la
multiplicación de un escalar por cada uno de los vectores que componen
la base $B_E$, podemos expresar $f(a)$
\begin{align}
f(a) =
\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_m\\
\end{pmatrix}
= y_1 v_1 + y_2 v_2 + \cdots x_m v_m
\end{align}

Así podemos plantear el siguiente sistema:
\begin{align}
\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_m\\
\end{pmatrix}
=
A \cdot
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{pmatrix}
\end{align}

Con esta matriz $A$ vemos que:
\begin{align}
A \cdot a = f(a)
\end{align}

Por lo tanto, podemos ver con esto, que toda aplicación lineal conduce
a una matriz.

*** Cambio de base
Para construir la matriz por la que se pregunta, tendremos que razonar
de que base a que base transforma la matriz ligada a la aplicación
lineal $P$.

Esta matriz nos da el vector respecto a la base canónica, por lo
tanto, si queremos usarla para pasar un vector desde la base $B$,
tendríamos primero que pasar dicho vector a la base canónica. Y si
luego queremos el resultado sobre la base $B$, tendremos que pasar
dicho resultado a esta base. Esto queda capturado en la siguiente
ecuación, donde $B_C$ es la base canónica y $M$ la matriz de cambio de
base con el subíndice indicando las bases de procedencia y destino:
\begin{align}
M_{B_C \rightarrow B} \cdot P \cdot M_{B \rightarrow B_C} \cdot \vec{v}
\end{align}

Sabemos que la matriz de cambio de base de la canónica a la otra base
es igual a la matriz en la que sus columnas son los vectores que
componen esa base. Y para hacer el cambio inverso, tendremos que usar
la inversa de esa matriz.

También, hemos visto que la matriz ligada a una aplicación lineal se
compone de la imagen de los vectores de la base. Como ya tenemos la
matriz $P$ y $U \oplus U^{\perp}$, vamos a calcular $M$ a partir de
estas matrices.
\begin{align}
M = B \cdot P \cdot B^{-1}
\end{align}

** Práctica
Para hallar la matriz simplemente ejecutamos el código similar al
descrito anteriormente.

#+name: figura-15
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-4>>
<<figura-14>>
<<figura-1>>
M1 = horzcat(A, X0, X1, X2, X3)
M = eval(mat2str(M1 * P * inv(M1), 3))
ans = M
#+end_src

#+RESULTS[ffe0b8a6739928b8481dd751fd57eaa0e0bfac64]: figura-15
|  -1.39 |   0.905 |   2.89 |  -1.43 |  -1.23 |  -0.357 |    1.18 |
|  -1.34 |    1.31 |   1.32 |  -1.06 | -0.892 |  -0.244 |   0.884 |
| -0.182 | -0.0734 |  0.999 | -0.274 | -0.251 | -0.0734 |   0.259 |
| 0.0893 | -0.0952 | -0.204 |  0.476 |  0.339 |   0.151 | -0.0893 |
| -0.101 |  0.0317 |  0.113 |  0.476 |  0.308 |   0.167 |  0.0536 |
| -0.646 |   0.415 |   1.05 |  0.226 | 0.0466 |   0.129 |   0.473 |
|  -1.54 |    1.05 |    2.6 | -0.274 | -0.443 |  0.0377 |    1.17 |

Para hallar el vector que nos pide simplemente tenemos que multiplicar
el vector por $M$:

#+name: figura-16
#+begin_src octave :noweb strip-export :exports both :cache yes
<<figura-15>>
b2 = [-9; 64; -33; 5; 0; 5; 10]
ans = M * b2
#+end_src

#+RESULTS[bc3cb4173440fc281a7469e4227bb7436c6d6fd0]: figura-16
|            -22.075 |
|  54.66000000000001 |
| -35.17359999999999 |
|  2.077499999999999 |
|             2.9598 |
|  4.228999999999997 |
|  5.778500000000005 |

\pagebreak
* Diagonalización de matrices
** Enunciado
Hallar una matriz invertible $S$ tal que (siendo $S^{-1}$ la inversa
de $S$) se tenga.

\begin{align}
S^{-1} P S = M
\end{align}

(El cálculo de $S^{-1}$ (la inversa de $S$) y el producto puede
hacerse con MATLAB).

Los valores propios de la matriz de proyección P son los elementos diagonales de M.
Relacionar este hecho con los vectores propios de P (y explicar por qué es natural que esos
vectores sean vectores propios).

** Teoría
Para diagonalizar matrices tenemos que entender el concepto de
diagonalización y para ello, también, el significado de autovalores y
autovectores.

*** Autovectores
Los autovectores de una matriz son aquellos vectores que cumplen la
característica de que cuando se multiplican por esa matriz, es como si
se hubiera multiplicado el vector por un escalar. O puesto en forma de
ecuación, donde $A$ es una matriz, $x$ un vector y $\lambda$ un
escalar:
\begin{align}
A \vec{x} = \lambda \vec{x}
\end{align}

En esta ecuación $x$ es un autovector y $\lambda$ un autovalor. Para
encontrar los autovectores y autovalores asociados a la matriz $A$,
tendremos que reescribir la ecuación de una forma que nos resulte más
fácil hacer cálculos:
\begin{align}
A \vec{x} &= \lambda \vec{x}\\
A \vec{x} - \lambda \vec{x} &= 0 \\
\left( A - \lambda I \right) \vec{x} &= 0
\end{align}

En esta ecuación $I$ es la matriz identidad del mismo orden que $A$,
recordamos que $\vec{x} = I \vec{x}$ y por lo tanto
$\lambda \vec{x} = \left( \lambda I \right) \vec{x}$. De la ecuación
anterior podemos extraer:
\begin{align}
\label{eigenvectproperties1}
\vec{x} \in \mathcal{N} \left( A - \lambda I \right)\\
\label{eigenvectproperties2}
\mathcal{N} \left( A - \lambda I \right) \neq \left\lbrace 0 \right\rbrace
\end{align}

Además de eso, $x \neq 0$, ya que cualquier matriz por $0$ siempre es
$0$.

Para que se cumpla la ecuación \ref{eigenvectproperties2}, se tiene
que cumplir que $A - \lambda I$ sea una matriz con una de las filas
linealmente dependiente de las otras, lo cual implica que su
determinante sea $0$, por lo tanto, para encontrar los autovalores
tendremos que resolver la ecuación de grado $n$ que resulte de la
siguiente formula:
\begin{align}
\text{det}\left( A - \lambda I \right) = 0
\end{align}

Luego, para hallar los autovectores, tendremos que operar para hallar
la solución al sistema dado por
$\left( A - \lambda I \right) \vec{x} = 0$.

*** Descomposición diagonal
Si queremos descomponer la matriz $M$ con su diagonal compuesta por
los autovalores de $P$ y el resto 0, de la siguiente forma:
\begin{align}
M = S^{-1} P S
\end{align}
Tendremos que ver la relación que guarda $S$ con $P$ y $M$.

De lo visto anteriormente sabemos que el autovector de una matriz
multiplicado por su autovalor es igual a el autovector multiplicando
a la matriz. Entonces, siendo $v_1$ a $v_n$ los autovectores de la
matriz $P$ y $\lambda_1$ a $\lambda_n$ sus respectivos autovalores,
podemos ver lo siguiente:
\begin{align}
P \cdot \left( v_1 | v_2 | \cdots | v_n \right) &=\\
\left( P \cdot v_1 | P \cdot v_2 | \cdots | P \cdot v_n \right) &=\\
\left( \lambda_1 \cdot v_1 | \lambda_2 \cdot v_2 | \cdots | \lambda_n \cdot v_n \right) &=\\
&= \left( v_1 | v_2 | \cdots | v_n \right)
\begin{pmatrix}
\lambda_1 & \          & \      & \         \\
\         & \lambda_2  & \      & \         \\
\         & \          & \ddots & \         \\
\         & \          & \      & \lambda_n
\end{pmatrix}
\end{align}
Por lo tanto, de aquí podemos extraer lo siguiente:
\begin{align}
P \cdot S &= S \cdot M \\
S^{-1} \cdot P \cdot S &= M
\end{align}
Donde $S = P \cdot \left( v_1 | v_2 | \cdots | v_n \right)$

\pagebreak
* Notas del texto
\theendnotes

[fn:1]Para ser más correcto, $E$ y $F$ se definen como espacios
vectoriales sobre el cuerpo $K$, esto significa que los vectores de
$E$ y $F$ están formados por los elementos que componen $K$ y que en
$K$ existen al menos dos operaciones $+$ y $\cdot$ (adición y
multiplicación) y se cumplen las siguientes propiedades:
1. $K$ es cerrado para $+$ y para $\cdot$
   a. $(a + b) \in K\ \ \forall a, b \in K$
   b. $(a \cdot b) \in K\ \ \forall a, b \in K$
2. La adición y multiplicación son asociativas en $K$:
   a. $(a + b) + c = a + (b + b)\ \ \forall a, b, c \in K$
   b. $(a \cdot b) \cdot c = a \cdot (b \cdot b)\ \ \forall a, b, c \in K$
3. La adición y multiplicación son conmutativas en $K$:
   a. $a + b = b + a\ \ \forall a, b \in K$
   b. $a \cdot b = b \cdot a\ \ \forall a, b \in K$
4. Existe un elemento neutro tanto para la adición como para la
   multiplicación:
   a. Para la adición es el $0$: $a + 0 = a\ \ \forall a \in K$
   b. Para la multiplicación es el $1$: $a \cdot 1 = a\ \ \forall a \in K$
5. Existe un inverso en cada operación que al operar el elemento con
   su inverso da igual al elemento neutro, para la adición se llama
   opuesto y para la multiplicación inverso:
   a. $a + (-a) = 0\ \ \forall a \in K$
   b. $a \cdot a^{-1} = 1\ \ \forall a \neq 0 \in K$
6. La multiplicación es distributiva sobre la adición
   a. $a \cdot (b + c) = (a \cdot b) + (a \cdot c)\ \ \forall a, b, c \in K$
7. El inverso de una multiplicación es igual a la multiplicación de
   los inversos
   a. $(a \cdot b)^{-1} = a^{-1} \cdot b^{-1}\ \ \forall a,b \in K$
8. $-a = (-1) \cdot a\ \ \forall a \in K$
9. $a \cdot 0 = 0\ \ \forall a \in K$
